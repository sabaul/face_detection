{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05217953-f84d-411b-a60f-79af93da7a1d",
   "metadata": {},
   "source": [
    "# 1. Setup and Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c98663f-6c3b-4460-bfe2-ded1b97ba5e0",
   "metadata": {},
   "source": [
    "## 1.1 Install Dependencies and Setup\n",
    "* labelme --> to annotate images and get json outputs\n",
    "* tensorflow and tensorflow-gpu --> You know why\n",
    "* opencv-python --> to access the smartphone camera or webcam, do real time detection\n",
    "* matplotlib --> plot images, bounding boxes and class (face for the current problem)\n",
    "* albumentation --> Do image augmentations and change the corresponding annotation coordinates (the labels) in the augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3897e43a-ff31-4579-87c6-eee0150ace5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install labelme tensorflow tensorflow-gpu opencv-python matplotlib albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be81021-e65b-42c2-920e-ccebc55d763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check tensorflow version\n",
    "# import tensorflow as tf\n",
    "# tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aa78c4-d838-43a0-8e44-ffb3ae0463bc",
   "metadata": {},
   "source": [
    "## 1.2 Collect Images Using OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d6fd68-eb8e-4fb3-8407-13db947cd70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef06ba0b-f502-43b3-b3d1-9cc7ba1725c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uuid to generate unique file names\n",
    "# just a sample of what uuid does\n",
    "uuid.uuid1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a894289-98d8-4f32-9595-613394874d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = os.path.join('data', 'images')\n",
    "number_images = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9270ac7c-bf9b-4df0-befe-c3db3886ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "for imgnum in range(number_images):\n",
    "    print(f\"Collecting image {imgnum}\")\n",
    "    ret, frame = cap.read()\n",
    "    imgname = os.path.join(IMAGES_PATH, f\"{str(uuid.uuid1())}.jpg\")\n",
    "    cv2.imwrite(imgname, frame)\n",
    "    cv2.imshow('Frame', frame)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0281be4-6006-4c0c-90e7-030746602545",
   "metadata": {},
   "source": [
    "## 1.3 Annotate Images with LabelMe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855e1290-8aeb-4807-b0d7-6cdd69672ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!labelme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6a5fce-eae1-4d13-a67e-9d35d60b730e",
   "metadata": {},
   "source": [
    "# 2. Review Dataset and Build Image Loading Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5787a3d-829d-41cb-9188-278c61220b4d",
   "metadata": {},
   "source": [
    "## 2.1 Import TF and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aef0a36-b074-43d0-880f-40a0a5edf74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5385b284-9b2c-480b-8dbc-93d9ca9601ba",
   "metadata": {},
   "source": [
    "## 2.2 Limit GPU Memory Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ffca34-55f4-4509-a174-889e16c2a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9c3e7b-3ec7-4ea6-858b-2c90bcc9e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa36bd4-7680-48c9-ac7c-89c0f55cbd34",
   "metadata": {},
   "source": [
    "## 2.3 Load Image into TF Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a49630-32fe-4701-9c53-aca7a4c62837",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = tf.data.Dataset.list_files(\"data\\\\images\\\\*.jpg\", shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d489ba35-3ecf-4235-b3e8-b346ee4a7727",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3e81d9-e13f-4be3-9a15-0582349f445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce567bc-b3bb-4f8f-b270-6c00456c0f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ff3283-848f-4891-ae50-47593541d2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(x):\n",
    "    byte_img = tf.io.read_file(x)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55c51d4-1ae8-4d60-b0d7-cbf1dcc61cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.map(load_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec563754-9618-4b62-9cab-db55773f0e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51154a0a-b476-4946-8cc7-988bca161899",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a0977c-37fa-4914-a630-ffaa726079d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b1562a-768f-41fe-981c-605f58626928",
   "metadata": {},
   "source": [
    "## 2.4 View Raw Images with Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e9ada8-c07c-4983-ae39-99633749afe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_generator = images.batch(4).as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe07a41-be36-440c-bdd3-97a0acf4aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images = image_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc01fdb-d9f5-4fb7-96f5-8b7826600295",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20, 20))\n",
    "for idx, image in enumerate(plot_images):\n",
    "    ax[idx].imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e715f609-f170-4396-9cfa-003081b57b6f",
   "metadata": {},
   "source": [
    "# 3. Partition Unaugmented Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c715565f-0460-4bee-b6dc-7dcbd1888ebb",
   "metadata": {},
   "source": [
    "## 3.1 Manually Split Data into Train, Test and Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a770a93b-015b-4721-bfc5-531692a6010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "117*0.1 # (12 for validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd6ac99-2199-4180-bee4-8d1da467096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "117 - 12 # (15 for test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261a70e2-eb49-499e-8985-cb61e65b470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "105-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af52c5d-53d3-497d-8825-5b023c20f804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rest in train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbcadcb-4f63-433f-a577-dfb9a97ea356",
   "metadata": {},
   "source": [
    "## 3.2 Move the Matching Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648de303-1a1d-4f9a-9675-7c9e28d03d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in ['train', 'test', 'val']:\n",
    "    for file in os.listdir(os.path.join('data', folder, 'images')):\n",
    "        filename = file.split('.')[0]+'.json'\n",
    "        existing_filepath = os.path.join('data', 'labels', filename)\n",
    "        if os.path.exists(existing_filepath):\n",
    "            new_filepath = os.path.join('data', folder, 'labels', filename)\n",
    "            os.replace(existing_filepath, new_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423600a9-ee59-4d00-b38a-8704e0bea920",
   "metadata": {},
   "source": [
    "# 4. Apply Image Augmentation on Images and Labels using Albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b476d0-c6ea-4906-ba3d-1d2bb1292c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(os.path.join('data', 'train', 'images', '05239bdd-ef90-11ec-95f8-d8bbc13d240a.jpg'))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e817bce-481c-473d-99a2-f4040015590b",
   "metadata": {},
   "source": [
    "## 4.1 Setup Albumentations Transform Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27f410d-9f48-4770-9936-60f9477db0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as alb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bc9198-801d-4709-8f36-b787f9571ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentor = alb.Compose([\n",
    "    alb.RandomCrop(width=450, height=450),\n",
    "    alb.HorizontalFlip(p=0.5),\n",
    "    alb.RandomBrightnessContrast(p=0.2),\n",
    "    alb.RandomGamma(p=0.2),\n",
    "    alb.RGBShift(p=0.2),\n",
    "    alb.VerticalFlip(p=0.5)\n",
    "], bbox_params=alb.BboxParams(format='albumentations',\n",
    "                              label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5babbc-3448-4b7c-b1ac-f5bea12b4023",
   "metadata": {},
   "source": [
    "## 4.2 Load a Test Image and Annotation with OpenCV and JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6e47df-8021-4ab8-85a3-fa066967e539",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(os.path.join('data', 'train', 'images', '05239bdd-ef90-11ec-95f8-d8bbc13d240a.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82891c68-473c-46ba-843b-88acc0d081e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data', 'train', 'labels', '05239bdd-ef90-11ec-95f8-d8bbc13d240a.json'), 'r') as f:\n",
    "    label = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba1f00b-bf65-4a46-9eb6-b8acc90680d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e8fbc5-1399-4344-b280-9c2a5e11f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "label['shapes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17344414-f2e3-44be-9c5b-d475dcf41d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label['shapes'][0]['points']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad444ca6-2fbb-48fa-9104-8c1c048351d2",
   "metadata": {},
   "source": [
    "## 4.3 Extract Coordinated and Rescale to Match Image Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80173b5c-acdc-49ab-bab3-baaf4032d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = [0, 0, 0, 0]\n",
    "coords[0] = label['shapes'][0]['points'][0][0]\n",
    "coords[1] = label['shapes'][0]['points'][0][1]\n",
    "coords[2] = label['shapes'][0]['points'][1][0]\n",
    "coords[3] = label['shapes'][0]['points'][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390e76c2-5898-4bfd-a28a-198610798da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6ed744-7800-4ebe-aa99-94cf3f2a275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = list(np.divide(coords, [640, 480, 640, 480]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8378395-584e-4771-80fd-3731b4f0539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b568a6ed-0407-4445-a7e0-09bdb5c0c5c6",
   "metadata": {},
   "source": [
    "## 4.4 Apply Augmentations and View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ef89f-bf2c-4b71-a2d7-405b31158e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented = augmentor(image=img, bboxes=[coords], class_labels=['face'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a84438-f1f6-45c1-a66a-e13b00384f92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaafab0-023f-4637-9aee-8a96f7cc8901",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented['bboxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a986361f-1d34-4797-a42f-8d0fff5415d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented['bboxes'][0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4c9bee-020e-471e-8ab7-92d60948f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.multiply(augmented['bboxes'][0][:2], [450, 450])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e580d0e-91f8-4330-946f-4c153dc28bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented['bboxes'][0][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bccd9c-4e52-4374-9aa9-b0129e0a0ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.multiply(augmented['bboxes'][0][2:], [450, 450])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb86687-c739-44e8-b6e3-a3caa69638d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.rectangle(\n",
    "    augmented['image'],\n",
    "    tuple(np.multiply(augmented['bboxes'][0][:2], [450, 450]).astype(int)),\n",
    "    tuple(np.multiply(augmented['bboxes'][0][2:], [450, 450]).astype(int)),\n",
    "    (255, 0, 0), 2)\n",
    "\n",
    "plt.imshow(augmented['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624c9638-c1b9-4b0b-bc00-1ecfb56ea0f5",
   "metadata": {},
   "source": [
    "# 5. Build and Run Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90820768-d0e4-4868-8e7d-0cbb0dd5c733",
   "metadata": {},
   "source": [
    "## 5.1 Run Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f0d5cb-50e1-4086-941f-29c706ba4c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix how many augmented images we need for one base image\n",
    "# For each base image, we will have 60 augmented image\n",
    "number_of_aug_images = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8decdc-3c16-4c95-ae53-1c3c9312b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for partition in ['train','test','val']: \n",
    "    for image in os.listdir(os.path.join('data', partition, 'images')):\n",
    "        img = cv2.imread(os.path.join('data', partition, 'images', image))\n",
    "\n",
    "        coords = [0,0,0.00001,0.00001]\n",
    "        label_path = os.path.join('data', partition, 'labels', f'{image.split(\".\")[0]}.json')\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                label = json.load(f)\n",
    "\n",
    "            coords[0] = label['shapes'][0]['points'][0][0]\n",
    "            coords[1] = label['shapes'][0]['points'][0][1]\n",
    "            coords[2] = label['shapes'][0]['points'][1][0]\n",
    "            coords[3] = label['shapes'][0]['points'][1][1]\n",
    "            coords = list(np.divide(coords, [640,480,640,480]))\n",
    "\n",
    "        try: \n",
    "            for x in range(60):\n",
    "                augmented = augmentor(image=img, bboxes=[coords], class_labels=['face'])\n",
    "                cv2.imwrite(os.path.join('aug_data', partition, 'images', f'{image.split(\".\")[0]}.{x}.jpg'), augmented['image'])\n",
    "\n",
    "                annotation = {}\n",
    "                annotation['image'] = image\n",
    "\n",
    "                if os.path.exists(label_path):\n",
    "                    if len(augmented['bboxes']) == 0: \n",
    "                        annotation['bbox'] = [0,0,0,0]\n",
    "                        annotation['class'] = 0 \n",
    "                    else: \n",
    "                        annotation['bbox'] = augmented['bboxes'][0]\n",
    "                        annotation['class'] = 1\n",
    "                else: \n",
    "                    annotation['bbox'] = [0,0,0,0]\n",
    "                    annotation['class'] = 0 \n",
    "\n",
    "\n",
    "                with open(os.path.join('aug_data', partition, 'labels', f'{image.split(\".\")[0]}.{x}.json'), 'w') as f:\n",
    "                    json.dump(annotation, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7180e3bc-427c-4a75-b8a6-43522ce506c7",
   "metadata": {},
   "source": [
    "## 5.2 Load Augmented Images to Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dd5563-2fe3-4fcb-8306-eae42cdad7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = tf.data.Dataset.list_files('aug_data\\\\train\\\\images\\\\*.jpg', shuffle=False)\n",
    "train_images = train_images.map(load_image)\n",
    "train_images = train_images.map(lambda x: tf.image.resize(x, (120, 120)))\n",
    "train_images = train_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d759c9f-3a28-4386-a49e-56e076608f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = tf.data.Dataset.list_files('aug_data\\\\test\\\\images\\\\*.jpg', shuffle=False)\n",
    "test_images = test_images.map(load_image)\n",
    "test_images = test_images.map(lambda x: tf.image.resize(x, (120, 120)))\n",
    "test_images = test_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3def63cd-8b67-40b6-80cf-b8cfc7ac65ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = tf.data.Dataset.list_files('aug_data\\\\val\\\\images\\\\*.jpg', shuffle=False)\n",
    "val_images = val_images.map(load_image)\n",
    "val_images = val_images.map(lambda x: tf.image.resize(x, (120, 120)))\n",
    "val_images = val_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dcc4db-5c98-4faa-b1e4-f9b413d8dee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05488037-efc2-42d6-a1d3-2356ebba45af",
   "metadata": {},
   "source": [
    "# 6. Prepare Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026e6a36-3ca9-4dd1-a4f6-9f5dd575332f",
   "metadata": {},
   "source": [
    "## 6.1 Build Label Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff115f2-471a-415e-8457-b614c216d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(label_path):\n",
    "    with open(label_path.numpy(), 'r', encoding='utf-8') as f:\n",
    "        label = json.load(f)\n",
    "    \n",
    "    return [label['class'], label['bbox']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0080c4-ac69-4f55-88a2-c6ec8d40f007",
   "metadata": {},
   "source": [
    "## 6.2 Load Labels to Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbf92d7-079d-49d4-b045-f8423f612d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tf.data.Dataset.list_files('aug_data\\\\train\\\\labels\\\\*.json', shuffle=False)\n",
    "train_labels = train_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7483a180-e8c8-41a7-97c3-b020f8969ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = tf.data.Dataset.list_files('aug_data\\\\test\\\\labels\\\\*.json', shuffle=False)\n",
    "test_labels = test_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b292d5fc-ea47-4ab8-aa87-fe737dbf38f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = tf.data.Dataset.list_files('aug_data\\\\val\\\\labels\\\\*.json', shuffle=False)\n",
    "val_labels = val_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1514df57-429e-4392-a042-11cafd47b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31f719d-a906-461d-bd84-36819313e50c",
   "metadata": {},
   "source": [
    "# 7. Combine Label and Image Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32df62af-7a23-45fa-a436-753b915f44bc",
   "metadata": {},
   "source": [
    "## 7.1 Check Partition Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1af8bb-2b7a-4d64-b004-baad0d219688",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_images), len(train_labels), len(test_images), len(test_labels), len(val_images), len(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8803a8-8816-4b4b-a2ef-8578461f8433",
   "metadata": {},
   "source": [
    "## 7.2 Create Final Datasets (Images/Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e016b1-445d-435e-923f-b03b1dcd50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.data.Dataset.zip((train_images, train_labels))\n",
    "train = train.shuffle(6000)\n",
    "train = train.batch(8)\n",
    "train = train.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b7d14-e3ce-4366-93b9-c0c8886262a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tf.data.Dataset.zip((test_images, test_labels))\n",
    "test = test.shuffle(1000)\n",
    "test = test.batch(8)\n",
    "test = test.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab754f15-c9c1-4bd1-a30d-3c3059bbf1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = tf.data.Dataset.zip((val_images, val_labels))\n",
    "val = val.shuffle(1000)\n",
    "val = val.batch(8)\n",
    "val = val.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143a40d6-803b-4209-a252-bc8594c361ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.as_numpy_iterator().next()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be48eb1c-1f6f-4b71-8571-f4d99062b72f",
   "metadata": {},
   "source": [
    "## 7.3 View images and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743eb47e-688e-4904-9cfa-f8fe2d9c71ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = train.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a03de4f-d10f-4f0c-b7a7-53fbb34bc46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = data_samples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce144b7a-8816-42f7-bba0-e97f8a80ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20, 20))\n",
    "for idx in range(4):\n",
    "    sample_image = res[0][idx]\n",
    "    sample_coords = res[1][1][idx]\n",
    "    \n",
    "    cv2.rectangle(\n",
    "        sample_image,\n",
    "        tuple(np.multiply(sample_coords[:2], [120, 120]).astype(int)),\n",
    "        tuple(np.multiply(sample_coords[2:], [120, 120]).astype(int)),\n",
    "        (255, 0, 0), 2)\n",
    "    \n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cf4fc1-f4db-4d8d-b0c9-05593a66f8ba",
   "metadata": {},
   "source": [
    "# 8. Build Deep Learning using Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfe72b3-f260-4fd2-8b27-6de7893c2bff",
   "metadata": {},
   "source": [
    "## 8.1 Import Layers and Base Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28584ccb-ce18-4f11-8183-e068fe185cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, GlobalMaxPooling2D\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c0b27-e025-4ea5-adf7-6d3ad65358e2",
   "metadata": {},
   "source": [
    "## 8.2 Download VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9690aa-d56a-40ac-a63c-2b7014835b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3e3a0d-f1c8-4475-bf90-e56f9a69b159",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7233fb9-104a-4cb8-b877-07c2dc299ad5",
   "metadata": {},
   "source": [
    "## 8.3 Build instance of Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd05bc9e-121b-4636-887e-8e481ce0d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_layer = Input(shape=(120, 120, 3))\n",
    "    vgg = VGG16(include_top=False)(input_layer)\n",
    "    \n",
    "    # Classification Model\n",
    "    f1 = GlobalMaxPooling2D()(vgg)\n",
    "    class1 = Dense(2048, activation='relu')(f1)\n",
    "    class2 = Dense(1, activation='sigmoid')(class1)\n",
    "    \n",
    "    # Bounding Box Model\n",
    "    f2 = GlobalMaxPooling2D()(vgg)\n",
    "    regress1 = Dense(2048, activation='relu')(f2)\n",
    "    regress2 = Dense(4, activation='sigmoid')(regress1)\n",
    "    \n",
    "    facetracker = Model(inputs=input_layer, outputs=[class2, regress2])\n",
    "    return facetracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d2f28-3e9c-43c1-a557-61732047e868",
   "metadata": {},
   "source": [
    "## 8.4 Test out Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3fa6a9-76fd-4b73-be72-792a478c4610",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f83ca5-b057-4691-b1b4-de26b227d30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd7e517-4ddc-4a91-a4f0-66a5813e13e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21391e1f-fd68-4dc0-a200-53346909bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96b693d-1d04-4d87-8815-fc542c9d1dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, coords = facetracker.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119be863-c652-4ca7-9a8c-7f40b6f361b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e94c1c9-636f-4c7c-bfb9-fa6e63bd7c93",
   "metadata": {},
   "source": [
    "# 9. Define Losses and Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e723e36-9fb0-495b-b19c-b9f0072f340a",
   "metadata": {},
   "source": [
    "## 9.1 Define Optimizer and LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdec1ea-4233-4fd5-8880-98dda86d3ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch = len(train)\n",
    "lr_decay = (1./0.75 - 1)/batches_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba17e38-6489-420a-8ba6-bc3eeb5591ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001, decay=lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6946a329-2381-4609-911d-c64df6f9e0a3",
   "metadata": {},
   "source": [
    "## 9.2 Create Localization Loss and Classification Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd54c28-bd9f-4393-9eae-14ca1c622b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_loss(y_true, yhat):\n",
    "    delta_coord = tf.reduce_sum(tf.square(y_true[:, :2] - yhat[:, :2]))\n",
    "    \n",
    "    h_true = y_true[:, 3] - y_true[:, 1]\n",
    "    w_true = y_true[:, 2] - y_true[:, 0]\n",
    "    \n",
    "    h_pred = yhat[:, 3] - yhat[:, 1]\n",
    "    w_pred = yhat[:, 2] - yhat[:, 0]\n",
    "    \n",
    "    delta_size = tf.reduce_sum(tf.square(w_true - w_pred) + tf.square(h_true - h_pred))\n",
    "    \n",
    "    return delta_coord + delta_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e16b2ca-1f78-42ee-ad49-4890d0c421df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "regress_loss = localization_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f6c64a-c8e5-4bdb-b243-a809e9ab3fc3",
   "metadata": {},
   "source": [
    "## 9.3 Test out Loss Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1ff820-5f61-4fed-9be3-0ad0d00fc148",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0], classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea09ddb1-820e-4c2b-b50e-559ae4819969",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[1], coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bedb6fa-8399-4bec-82d2-13f7a32a7d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "localization_loss(y[1], coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d2729c-e0fc-4035-b2cb-7ff74654f7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_loss(y[0], classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93081db4-b0ba-4856-b627-76af4c1a551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "regress_loss(y[1], coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa1b4bf-99a3-466e-8a88-a09f49e2b438",
   "metadata": {},
   "source": [
    "# 10. Train Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1a4f0f-c1d2-4236-82d1-7dec6250e0cf",
   "metadata": {},
   "source": [
    "## 10.1 Create Custom Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bc45e4-9fbf-4390-9efd-58380ecc57db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceTracker(Model):\n",
    "    def __init__(self, eyetracker, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = eyetracker\n",
    "    \n",
    "    def compile(self, opt, class_loss, localization_loss, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.closs = class_loss\n",
    "        self.lloss = localization_loss\n",
    "        self.opt = opt\n",
    "    \n",
    "    def train_step(self, batch, **kwargs):\n",
    "        X, y = batch\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            classes, coords = self.model(X, training=True)\n",
    "            \n",
    "            batch_classloss = self.closs(y[0], classes)\n",
    "            batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "            \n",
    "            total_loss = batch_localizationloss + 0.5*batch_classloss\n",
    "            \n",
    "            grad = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "        \n",
    "        opt.apply_gradients(zip(grad, self.model.trainable_variables))\n",
    "        \n",
    "        return {\"total_loss\": total_loss, \"class_loss\": batch_classloss, \"regress_loss\":batch_localizationloss}\n",
    "    \n",
    "    def test_step(self, batch, **kwargs):\n",
    "        X, y = batch\n",
    "        \n",
    "        classes, coords = self.model(X, training=False)\n",
    "        \n",
    "        batch_classloss = self.closs(y[0], classes)\n",
    "        batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "        total_loss = batch_localizationloss + 0.5*batch_classloss\n",
    "        \n",
    "        return {\"total_loss\": total_loss, \"class_loss\": batch_classloss, \"regress_loss\": batch_localizationloss}\n",
    "    \n",
    "    def call(self, X, **kwargs):\n",
    "        return self.model(X, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7314ae6-b90c-4089-af02-b08c8fa6be63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FaceTracker(facetracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbd1a8f-b2ee-47aa-be71-dee7b79677b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(opt, class_loss, regress_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deef784-395d-4ae2-a0ae-842325b4a000",
   "metadata": {},
   "source": [
    "## 10.2 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6449345-5948-4423-a115-c07cd382d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4bdd64-2749-486a-9a97-517b45db269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be6b19b-0109-4864-8134-1743b5ef8287",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train, epochs=10, validation_data=val, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af3e377-6408-44f4-97b7-48905969505e",
   "metadata": {},
   "source": [
    "## 10.3 Plot Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9933a1-4e01-49c6-9a0d-57693dee2c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da87838-39be-418c-a48d-9b4dbf594b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(20, 5))\n",
    "\n",
    "ax[0].plot(hist.history['total_loss'], color='teal', label='loss')\n",
    "ax[0].plot(hist.history['val_total_loss'], color='orange', label='val loss')\n",
    "ax[0].title.set_text('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(hist.history['class_loss'], color='teal', label='class_loss')\n",
    "ax[1].plot(hist.history['val_class_loss'], color='orange', label='val class loss')\n",
    "ax[1].title.set_text('Classification Loss')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(hist.history['regress_loss'], color='teal', label='regress loss')\n",
    "ax[2].plot(hist.history['val_regress_loss'], color='orange', label='val regress loss')\n",
    "ax[2].title.set_text('Regression Loss')\n",
    "ax[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406cc4f5-a3bc-481f-bab2-c04d2c6f64c4",
   "metadata": {},
   "source": [
    "# 11. Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5fb0b7-9a32-4fcf-8b70-55ab7268c7cc",
   "metadata": {},
   "source": [
    "## 11.1 Make Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e6c060-eb4f-42b0-bd18-b6fbf5e6a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9acf62a-80ce-49b2-8438-51c9be2ab7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test_data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159a4ff5-79cc-47cf-91d3-52a29c2b9d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = facetracker.predict(test_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e24444-a27d-4037-8965-b2416054c0d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20, 20))\n",
    "for idx in range(4):\n",
    "    sample_image = test_sample[0][idx]\n",
    "    sample_coords = yhat[1][idx]\n",
    "    \n",
    "    if yhat[0][idx] > 0.9:\n",
    "        cv2.rectangle(\n",
    "            sample_image,\n",
    "            tuple(np.multiply(sample_coords[:2], [120, 120]).astype(int)),\n",
    "            tuple(np.multiply(sample_coords[2:], [120, 120]).astype(int)),\n",
    "            (255, 0, 0), 2)\n",
    "    \n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a6c942-5407-4c0f-a40f-3a816a712bb0",
   "metadata": {},
   "source": [
    "## 11.2 Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b2c11-569c-4d75-90c9-2f83240e64a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33250ee5-7870-4045-9183-869054b003ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker.save('facetracker_jun19.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e1fed8-1241-4761-9426-d35a883aeaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker = load_model('facetracker_jun19.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f714388-2132-47aa-b93c-f656ae2be93a",
   "metadata": {},
   "source": [
    "## 11.3 Real Time Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20408256-dd59-4585-b864-c5db50f2621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    _ , frame = cap.read()\n",
    "    frame = frame[50:500, 50:500,:]\n",
    "    \n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    resized = tf.image.resize(rgb, (120,120))\n",
    "    \n",
    "    yhat = facetracker.predict(np.expand_dims(resized/255,0))\n",
    "    sample_coords = yhat[1][0]\n",
    "    \n",
    "    if yhat[0] > 0.5: \n",
    "        # Controls the main rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.multiply(sample_coords[:2], [450,450]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [450,450]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "        # Controls the label rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int), \n",
    "                                    [0,-30])),\n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                    [80,0])), \n",
    "                            (255,0,0), -1)\n",
    "        \n",
    "        # Controls the text rendered\n",
    "        cv2.putText(frame, 'face', tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                               [0,-5])),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow('EyeTrack', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760176c4-7c57-42b1-bc6f-722c7d5f3a34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
